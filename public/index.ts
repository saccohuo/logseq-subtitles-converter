/// <reference path="./logseq.d.ts" />

import "@logseq/libs";
import { setup as l10nSetup, t } from "logseq-l10n"; //https://github.com/sethyuan/logseq-l10n
import ja from "./translations/ja.json";
import { IBatchBlock, IHookEvent } from "@logseq/libs/dist/LSPlugin.user";

// å®šä¹‰è½¬å½•é€‰é¡¹æ¥å£
interface TranscriptionOptions {
  modelType: 'whisper' | 'funasr';  // æ¨¡å‹ç±»å‹ï¼šwhisper æˆ– funasr
  whisperModelSize?: string;        // Whisper æ¨¡å‹å¤§å°
  minLength?: string;               // æœ€å°æ®µè½é•¿åº¦
  zhType?: string;                  // ä¸­æ–‡ç±»å‹ï¼ˆç®€ä½“æˆ–ç¹ä½“ï¼‰
  funasrModelName?: string;         // FunASR æ¨¡å‹åç§°
  funasrModelSource?: string;       // FunASR æ¨¡å‹æº
  ollamaModel?: string;
  ollamaEndpoint?: string;
  segmentModel?: string;
  openaiPriority?: string;
  enableOpenaiRotation?: boolean;
  useSharedOpenAIApiKey?: boolean;
  sharedOpenAIApiKey?: string;
  useSharedOpenAIApiEndpoint?: boolean;
  sharedOpenAIApiEndpoint?: string;
  openaiSettings?: Array<{
    apiKey: string;
    model: string;
    apiEndpoint: string;
    maxSegmentLength: number;
  }>;
  performSegmentation?: boolean;
  defaultMaxSegmentLength?: number;
  segmentationTolerance?: number;
  segmentationToleranceUnit?: string;
  summarizeBlockContent?: string;
  summarizeOpenAISettingPriority?: string;
}

// å®šä¹‰è½¬å½•æ®µè½æ¥å£
interface TranscriptionSegment {
  segment: string;  // è½¬å½•æ–‡æœ¬
  startTime: number;  // å¼€å§‹æ—¶é—´
  endTime?: number;  // ç»“æŸæ—¶é—´ï¼ˆå¯é€‰ï¼Œå› ä¸º Whisper å¯èƒ½ä¸æä¾›ï¼‰
}

// å®šä¹‰è½¬å½•å“åº”æ¥å£
interface TranscriptionResponse {
  segments: {
    start: number;
    text: string;
  }[];
  source?: string;
  error?: string;
}

// æ–°å¢çš„é€šç”¨ formData å¤„ç†å‡½æ•°
function appendCommonFormData(formData: FormData, options: TranscriptionOptions) {
  formData.append('model_type', options.modelType);
  formData.append('perform_segmentation', options.performSegmentation ? 'true' : 'false');
  formData.append('segment_model', options.segmentModel || 'ollama');
  formData.append('max_length', (options.defaultMaxSegmentLength || 1500).toString());
  formData.append('segmentation_tolerance', (options.segmentationTolerance || 5).toString());
  formData.append('segmentation_tolerance_unit', options.segmentationToleranceUnit || 'percent');

  if (options.segmentModel === 'openai') {
    formData.append('enable_openai_rotation', options.enableOpenaiRotation ? 'true' : 'false');
    formData.append('use_shared_openai_api_key', options.useSharedOpenAIApiKey ? 'true' : 'false');
    formData.append('use_shared_openai_api_endpoint', options.useSharedOpenAIApiEndpoint ? 'true' : 'false');
    
    if (options.useSharedOpenAIApiKey) {
      formData.append('shared_openai_api_key', options.sharedOpenAIApiKey || '');
    }
    if (options.useSharedOpenAIApiEndpoint) {
      formData.append('shared_openai_api_endpoint', options.sharedOpenAIApiEndpoint || '');
    }
    
    for (let i = 1; i <= 5; i++) {
      const apiKey = options.useSharedOpenAIApiKey ? options.sharedOpenAIApiKey : options.openaiSettings?.[i-1]?.apiKey;
      const endpoint = options.useSharedOpenAIApiEndpoint ? options.sharedOpenAIApiEndpoint : options.openaiSettings?.[i-1]?.apiEndpoint;
      const model = options.openaiSettings?.[i-1]?.model;
      const maxSegmentLength = options.openaiSettings?.[i-1]?.maxSegmentLength;
      
      if (apiKey && endpoint) {
        formData.append(`openai_api_key${i}`, apiKey);
        formData.append(`openai_api_endpoint${i}`, endpoint);
        formData.append(`openai_models${i}`, model || '');
      }
      formData.append(`openai_max_segment_length${i}`, (maxSegmentLength || 0).toString());
    }
    
    formData.append('openai_priority', options.openaiPriority || '1,2,3,4,5');
  } else if (options.segmentModel === 'ollama') {
    formData.append('ollama_model', options.ollamaModel || logseq.settings?.ollamaModel || "qwen2.5:3b");
    formData.append('ollama_endpoint', options.ollamaEndpoint || logseq.settings?.ollamaEndpoint || "http://localhost:11434");
    formData.append('ollama_max_segment_length', (logseq.settings?.ollamaMaxSegmentLength || 0).toString());
  }
}

// è·å–è½¬å½•è®¾ç½®
export function getTranscriptionSettings(): TranscriptionOptions {
  const modelType = (logseq.settings?.["modelType"] as 'whisper' | 'funasr') || "whisper";
  const whisperModelSize = logseq.settings?.["whisperModelSize"] as string;
  const minLength = logseq.settings?.["minLength"] as string;
  const zhType = logseq.settings?.["zhType"] as string;
  const funasrModelName = logseq.settings?.["funasrModelName"] as string;
  const funasrModelSource = logseq.settings?.["funasrModelSource"] as string;
  const ollamaModel = logseq.settings?.["ollamaModel"] as string;
  const ollamaEndpoint = logseq.settings?.["ollamaEndpoint"] as string;
  const segmentModel = logseq.settings?.["segmentModel"] as string;
  const openaiPriority = logseq.settings?.["openaiPriority"] as string;
  const enableOpenaiRotation = logseq.settings?.["enableOpenaiRotation"] as boolean;
  const useSharedOpenAIApiKey = logseq.settings?.["useSharedOpenAIApiKey"] as boolean;
  const sharedOpenAIApiKey = logseq.settings?.["sharedOpenAIApiKey"] as string;
  const useSharedOpenAIApiEndpoint = logseq.settings?.["useSharedOpenAIApiEndpoint"] as boolean;
  const sharedOpenAIApiEndpoint = logseq.settings?.["sharedOpenAIApiEndpoint"] as string;
  const performSegmentation = logseq.settings?.["performSegmentation"] as boolean;
  const defaultMaxSegmentLength = logseq.settings?.["defaultMaxSegmentLength"] as number;
  const segmentationTolerance = logseq.settings?.["segmentationTolerance"] as number;
  const segmentationToleranceUnit = logseq.settings?.["segmentationToleranceUnit"] as string;
  const summarizeBlockContent = logseq.settings?.["summarizeBlockContent"] as string;
  const summarizeOpenAISettingPriority = logseq.settings?.["summarizeOpenAISettingPriority"] as string;

  const openaiSettings = [];
  for (let i = 1; i <= 5; i++) {
    openaiSettings.push({
      apiKey: logseq.settings?.[`openaiApiKey${i}`] as string,
      model: logseq.settings?.[`openaiModel${i}`] as string,
      apiEndpoint: logseq.settings?.[`openaiApiEndpoint${i}`] as string,
      maxSegmentLength: logseq.settings?.[`openaiMaxSegmentLength${i}`] as number,
    });
  }

  return {
    modelType,
    whisperModelSize,
    minLength,
    zhType,
    funasrModelName,
    funasrModelSource,
    ollamaModel,
    ollamaEndpoint,
    segmentModel,
    openaiPriority,
    enableOpenaiRotation,
    useSharedOpenAIApiKey,
    sharedOpenAIApiKey,
    useSharedOpenAIApiEndpoint,
    sharedOpenAIApiEndpoint,
    openaiSettings,
    performSegmentation,
    defaultMaxSegmentLength,
    segmentationTolerance,
    segmentationToleranceUnit,
    summarizeBlockContent,
    summarizeOpenAISettingPriority,
  };
}

console.log("Plugin script started loading");

// åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ è¿™è¡Œ
// let globalWhisperLocalEndpoint: string | undefined;

// åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ è¿™ä¸ªå£°æ˜
declare global {
  interface Window {
    whisperSubtitlesPlugin: {
      getSetting: (key: string) => any;
      getAllSettings: () => any;
    };
  }
}

// åœ¨ main å‡½æ•°ä¸­æˆ– logseq.ready å›è°ƒä¸­æ·»åŠ 
window.whisperSubtitlesPlugin = {
  getSetting: (key: string) => logseq.settings?.[key],
  getAllSettings: () => logseq.settings
};

// å°† getSetting å‡½æ•°å®šä¹‰ä¸ºå…¨å±€å‡½æ•°
function getSetting(key: string): any {
  return logseq.settings?.[key];
}

// ä¸»å‡½æ•°
async function main() {
  console.log("Main function started");
  
  await l10nSetup({ builtinTranslations: { ja } });
  console.log("l10n setup completed");
  
  registerSettings();
  
  registerCommands();
  console.log("Plugin initialization completed");

  // åˆ é™¤ Ollama æ¨¡å‹è·å–ç›¸å…³çš„ä»£ç 

  logseq.onSettingsChanged((newSettings, oldSettings) => {
    // åˆ é™¤ Ollama ç›¸å…³çš„è®¾ç½®å˜æ›´å¤„ç†
  });

  // åˆ é™¤ logseq.provideModel è°ƒç”¨

  // è·å–å¹¶è®¾ç½®å›¾è°±è·¯å¾„
  const graph = await logseq.App.getCurrentGraph();
  if (graph) {
    await logseq.updateSettings({ graphPath: graph.path });
  }
}

function registerSettings() {
  console.log("Registering plugin settings");
  
  const openaiSettingsGroup = [
    {
      key: "openaiPriority",
      type: "string",
      default: "1,2,3,4,5",
      title: t("OpenAI API Priority"),
      description: t("Set the priority for OpenAI API settings (1-5, separated by comma, priority from high to low)"),
    },
    {
      key: "enableOpenaiRotation",
      type: "boolean",
      default: false,
      title: t("Enable OpenAI API Rotation"),
      description: t("Rotate through OpenAI API settings if text segmentation fails"),
    },
    {
      key: "useSharedOpenAIApiKey",
      type: "boolean",
      default: false,
      title: t("Use Shared OpenAI API Key"),
      description: t("Use shared OpenAI API Key for all configurations"),
    },
    {
      key: "sharedOpenAIApiKey",
      type: "string",
      default: "",
      title: t("Shared OpenAI API Key"),
      description: t("Shared API key for all OpenAI configurations"),
      visibility: "useSharedOpenAIApiKey === true",
    },
    {
      key: "useSharedOpenAIApiEndpoint",
      type: "boolean",
      default: false,
      title: t("Use Shared OpenAI API Endpoint"),
      description: t("Use shared OpenAI API Endpoint for all configurations"),
    },
    {
      key: "sharedOpenAIApiEndpoint",
      type: "string",
      default: "https://api.openai.com/v1",
      title: t("Shared OpenAI API Endpoint"),
      description: t("Shared API endpoint for all OpenAI configurations"),
      visibility: "useSharedOpenAIApiEndpoint === true",
    },
  ];

  // ä¸ºæ¯ç»„ OpenAI è®¾ç½®æ·»åŠ  maxSegmentLength
  for (let i = 1; i <= 5; i++) {
    openaiSettingsGroup.push(
      {
        key: `openaiApiKey${i}`,
        type: "string",
        default: "",
        title: t(`OpenAI API Key ${i}`),
        description: t(`API key for OpenAI setting ${i}`),
        visibility: "useSharedOpenAIApiKey === false",
      },
      {
        key: `openaiApiEndpoint${i}`,
        type: "string",
        default: "https://api.openai.com/v1",
        title: t(`OpenAI API Endpoint ${i}`),
        description: t(`API endpoint for OpenAI setting ${i}`),
        visibility: "useSharedOpenAIApiEndpoint === false",
      },
      {
        key: `openaiModel${i}`,
        type: "string",
        default: "gpt-3.5-turbo",
        title: t(`OpenAI Model ${i}`),
        description: t(`Model to use for OpenAI setting ${i}`),
      },
      {
        key: `openaiMaxSegmentLength${i}`,
        type: "number",
        default: 0,
        title: t(`OpenAI Max Segment Length ${i}`),
        description: t(`Maximum segment length for OpenAI setting ${i} (0 to use default)`),
      }
    );
  }

  logseq.useSettingsSchema([
    {
      key: 'group_general',
      title: "ğŸ›ï¸ General Settings",
      description: "",
      type: "heading",
      default: null,
    },
    {
      key: "modelType",
      type: "enum",
      default: "whisper",
      enumChoices: ["whisper", "funasr"],
      title: t("Speech recognition model"),
      description: t("Choose between Whisper and FunASR models"),
    },
    {
      key: "whisperLocalEndpoint",
      type: "string",
      default: "http://127.0.0.1:5014",
      title: t("End point of logseq-whisper-subtitles-server"),
      description: t("default: http://127.0.0.1:5014"),
    },
    {
      key: "youtubeOutputFormat",
      type: "string",
      default: "{{youtube-timestamp <start>}} <text>",
      title: t("YouTube Output Format"),
      description: t("Format for YouTube transcription output. Use <start> for timestamp and <text> for transcribed text"),
    },
    {
      key: "otherVideoOutputFormat",
      type: "string",
      default: "{{renderer :media-timestamp, <start>}} <text>",
      title: t("Other Video Output Format"),
      description: t("Format for non-YouTube video transcription output. Need logseq-media-ts plugin. Use <start> for timestamp and <text> for transcribed text"),
    },
    {
      key: "grandparentBlockTitle",
      type: "string",
      default: "",
      title: t("Grandparent Block Title"),
      description: t("Title for the grandparent block of transcription. Leave empty to skip creating this block."),
    },
    {
      key: "parentBlockTitle",
      type: "string",
      default: "",
      title: t("Parent Block Title"),
      description: t("Title for the parent block of transcription. Leave empty to skip creating this block."),
    },
    {
      key: 'group_whisper',
      title: "ğŸ—£ï¸ Whisper Settings",
      description: "",
      type: "heading",
      default: null,
    },
    {
      key: "whisperModelSize",
      type: "enum",
      default: "base",
      enumChoices: ["tiny", "base", "small", "medium", "large"],
      title: t("Whisper model size"),
      description: t("Only applicable for Whisper model"),
      visibility: "modelType === 'whisper'",
    },
    {
      key: "minLength",
      type: "number",
      default: 100,
      title: t("Minimum length of a segment"),
      description: t("if set to zero, segments will be split by .?!, otherwise, segments less than minLength will be merged"),
      visibility: "modelType === 'whisper'",
    },
    {
      key: "zhType",
      type: "enum",
      default: "zh-cn",
      enumChoices: ["zh-cn", "zh-tw"],
      title: t("Chinese language type"),
      description: "zh-cn and zh-tw",
      visibility: "modelType === 'whisper'",
    },
    {
      key: 'group_funasr',
      title: "ğŸ™ï¸ FunASR Settings",
      description: "",
      type: "heading",
      default: null,
    },
    {
      key: "funasrModelName",
      type: "enum",
      default: "SenseVoiceSmall",
      enumChoices: ["paraformer-zh", "paraformer-en", "Qwen-Audio", "Qwen-Audio-Chat", "emotion2vec+large"],
      title: t("FunASR model"),
      description: t("Choose FunASR model"),
      visibility: "modelType === 'funasr'",
    },
    {
      key: "funasrModelSource",
      type: "enum",
      default: "modelscope",
      enumChoices: ["modelscope", "huggingface"],
      title: t("FunASR model source"),
      description: t("Choose FunASR model source"),
      visibility: "modelType === 'funasr'",
    },
    {
      key: "hotwordFilePath",
      type: "string",
      default: "",
      title: t("Hotword File Path"),
      description: t("Path to the hotword file (optional)"),
      visibility: "modelType === 'funasr'",
    },
    {
      key: 'group_segmentation',
      title: "âœ‚ï¸ Text Segmentation Settings",
      description: "",
      type: "heading",
      default: null,
    },
    {
      key: "performSegmentation",
      type: "boolean",
      default: false,
      title: t("Perform text segmentation"),
      description: t("Choose whether to perform text segmentation after transcription"),
    },
    {
      key: "segmentModel",
      type: "enum",
      default: "ollama",
      enumChoices: ["ollama", "openai"],
      title: t("Text Segmentation Model"),
      description: t("Choose the model for text segmentation"),
      visibility: "performSegmentation === true",
    },
    {
      key: "defaultMaxSegmentLength",
      type: "number",
      default: 1500,
      title: t("Default maximum segment length"),
      description: t("Default maximum number of characters per segment"),
    },
    {
      key: "segmentationToleranceUnit",
      type: "enum",
      default: "percent",
      enumChoices: ["percent", "characters"],
      title: t("Segmentation tolerance unit"),
      description: t("Unit for segmentation tolerance (percentage or characters)"),
    },
    {
      key: "segmentationTolerance",
      type: "number",
      default: 5,
      title: t("Segmentation tolerance"),
      description: t("Acceptable difference in text length after segmentation (percentage)"),
    },
    {
      key: 'group_ollama',
      title: "ğŸ¤– Ollama Settings",
      description: "",
      type: "heading",
      default: null,
      visibility: "segmentModel === 'ollama'",
    },
    {
      key: "ollamaEndpoint",
      type: "string",
      default: "http://localhost:11434",
      title: t("Ollama API Endpoint"),
      description: t("Endpoint for Ollama API"),
      visibility: "segmentModel === 'ollama'",
    },
    {
      key: "ollamaModel",
      type: "string",
      default: "qwen2.5:3b",
      title: t("Ollama Model"),
      description: t("Model to use for text segmentation. Enter the model name manually."),
      visibility: "segmentModel === 'ollama'",
    },
    {
      key: "ollamaMaxSegmentLength",
      type: "number",
      default: 0,
      title: t("Ollama maximum segment length"),
      description: t("Maximum number of characters per segment for Ollama (0 to use default)"),
      visibility: "segmentModel === 'ollama'",
    },
    {
      key: 'group_openai',
      title: "ğŸ§  OpenAI API Settings",
      description: "",
      type: "heading",
      default: null,
      visibility: "segmentModel === 'openai'",
    },
    ...openaiSettingsGroup,
    {
      key: 'group_summarize',
      title: "ğŸ—‚ï¸ Summarize Settings",
      type: "heading",
      default: null,
    },
    {
      key: "summarizeBlockContent",
      type: "string",
      default: "#è§†é¢‘æ€»ç»“",
      title: t("Summarize Block Content"),
      description: t("Content of the block to insert before the summary"),
    },
    {
      key: "summarizeOpenAISettingPriority",
      type: "string",
      default: "1,2,3,4,5",
      title: t("OpenAI API Setting Priority for Summarization"),
      description: t("Choose the priority of OpenAI API settings to use for summarization (comma-separated, e.g., 1,2,3,4,5). Note: This does not use segment_length information."),
    },
  ]);

  console.log("Plugin settings registered");
  console.log("Settings immediately after registration:", logseq.settings);
}

function registerCommands() {
  console.log("Registering plugin commands");
  
  logseq.Editor.registerSlashCommand(t("transcribe-subtitle"), runTranscription);
  logseq.Editor.registerBlockContextMenuItem(t("transcribe-subtitle"), runTranscription);
  
  // æ³¨å†Œæ–°çš„å‘½ä»¤
  logseq.Editor.registerBlockContextMenuItem(t("transcribe from subtitle file(Youtube)"), (e) => transcribeFromSubtitleFile(e, false, 'youtube'));
  logseq.Editor.registerBlockContextMenuItem(t("transcribe from subtitle file(other video)"), (e) => transcribeFromSubtitleFile(e, false, 'other'));
  logseq.Editor.registerBlockContextMenuItem(t("transcribe from subtitle file after segment(Youtube)"), (e) => transcribeFromSubtitleFile(e, true, 'youtube'));
  logseq.Editor.registerBlockContextMenuItem(t("transcribe from subtitle file after segment(other video)"), (e) => transcribeFromSubtitleFile(e, true, 'other'));
  
  // æ·»åŠ æ–°çš„ "summarize subtitle" å‘½ä»¤
  logseq.Editor.registerBlockContextMenuItem(t("summarize subtitle"), summarizeSubtitle);
  
  console.log("Plugin commands registered");
}

// è¿è¡Œè½¬å½•
export async function runTranscription(b: IHookEvent) {
  const currentBlock = await logseq.Editor.getBlock(b.uuid);
  if (currentBlock) {
    const settings = getTranscriptionSettings();
    
    // æ˜¾ç¤ºå¤„ç†ä¸­çš„ UI
    showProcessingUI(currentBlock.uuid);

    try {
      let hotwords = '';
      // åªæœ‰åœ¨ FunASR æ¨¡å¼ä¸‹æ‰æ˜¾ç¤ºçƒ­è¯å¼¹çª—
      if (settings.modelType === 'funasr') {
        hotwords = await showHotwordsInputDialog();
        console.log("Hotwords:", hotwords);
      }

      // æ‰§è¡Œè½¬å½•
      const transcription = await transcribeContent(currentBlock.content, settings, hotwords);
      if (transcription.error) {
        logseq.UI.showMsg(transcription.error, "error");
        return;
      }

      // æ’å…¥è½¬å½•ç»“æœ
      await insertTranscription(currentBlock, transcription);
      logseq.UI.showMsg(t("Transcription completed and inserted"), "success");
    } catch (e: any) {
      logseq.UI.showMsg(t("Transcription failed: ") + e.message, "error");
    } finally {
      // éšè—å¤„ç†ä¸­çš„ UI
      hideProcessingUI();
    }
  }
}

// è½¬å½•å†…å®¹
async function transcribeContent(content: string, options: TranscriptionOptions, hotwords: string): Promise<TranscriptionResponse> {
  const baseEndpoint = logseq.settings?.whisperLocalEndpoint || "http://127.0.0.1:5014";
  const endpoint = `${baseEndpoint}/transcribe`;
  
  const formData = new FormData();
  formData.append('text', content);
  formData.append('hotwords', hotwords);  // æ·»åŠ çƒ­è¯åˆ°è¡¨å•æ•°æ®
  appendCommonFormData(formData, options);
  
  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      console.error("Server response:", response.status, response.statusText);
      const responseText = await response.text();
      console.error("Response body:", responseText);
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const result = await response.json();

    // å¤„ç† OpenAI API è½®è¯¢é€šçŸ¥
    if (result.openai_rotation_message) {
      handleOpenAIRotationNotification(result.openai_rotation_message);
    }

    console.log("Perform segmentation:", logseq.settings?.performSegmentation || "No");
    console.log("Transcription source:", result.source);
    console.log("Hotwords used:", hotwords);  // è®°å½•ä½¿ç”¨çš„çƒ­è¯

    return {
      segments: result.segments,
      source: result.source,
    };
  } catch (error) {
    console.error("Fetch error:", error);
    throw error;
  }
}

// æ’å…¥è½¬å½•ç»“æœ
async function insertTranscription(block: any, transcription: TranscriptionResponse) {
  console.log("Inserting transcription:", transcription);
  const youtubeOutputFormat = logseq.settings?.youtubeOutputFormat || "{{youtube-timestamp <start>}} <text>";
  const otherVideoOutputFormat = logseq.settings?.otherVideoOutputFormat || "{{renderer :media-timestamp, <start>}} <text>";
  const parentBlockTitle = logseq.settings?.parentBlockTitle || "";
  const grandparentBlockTitle = logseq.settings?.grandparentBlockTitle || "";
  
  let insertionBlock = block;
  
  if (grandparentBlockTitle && parentBlockTitle) {
    // 1. åˆ›å»ºç¥–çˆ¶å—ï¼ˆä¸è§†é¢‘å—åŒçº§
    let grandparentBlock = await logseq.Editor.insertBlock(block.uuid, grandparentBlockTitle, { sibling: true });
    if (!grandparentBlock) {
      throw new Error("Failed to create grandparent block");
    }
    
    // 2. åˆ›å»ºçˆ¶å—ï¼ˆä½œä¸ºç¥–çˆ¶å—çš„å­å—ï¼‰
    let parentBlock = await logseq.Editor.insertBlock(grandparentBlock.uuid, parentBlockTitle, { sibling: false });
    if (!parentBlock) {
      throw new Error("Failed to create parent block");
    }
    insertionBlock = parentBlock;
  } else if (grandparentBlockTitle || parentBlockTitle) {
    // å¦‚æœåªæœ‰ä¸€ä¸ªéç©ºï¼Œåˆ›å»ºä¸€ä¸ªå—ï¼ˆä¸è§†é¢‘å—åŒçº§ï¼‰
    const blockTitle = grandparentBlockTitle || parentBlockTitle;
    let newBlock = await logseq.Editor.insertBlock(block.uuid, blockTitle, { sibling: true });
    if (!newBlock) {
      throw new Error("Failed to create block");
    }
    insertionBlock = newBlock;
  }
  // å¦‚æœä¸¤ä¸ªéƒ½ä¸ºç©ºï¼ŒinsertionBlock ä¿æŒä¸ºåŸå§‹çš„ block

  // 3. æ’å…¥è½¬å½•å†…å®¹
  const outputFormat = transcription.source === "youtube" ? youtubeOutputFormat : otherVideoOutputFormat;
  const blocks: IBatchBlock[] = transcription.segments.map(segment => ({
    content: outputFormat
      .replace('<start>', formatTime(segment.start))
      .replace('<text>', segment.text),
  }));
  console.log("Blocks:", blocks);
  await logseq.Editor.insertBatchBlock(insertionBlock.uuid, blocks, { sibling: false });
}


// æ ¼å¼åŒ–æ—¶é—´
function formatTime(seconds: number): string {
  if (typeof seconds !== 'number' || isNaN(seconds) || seconds < 0) {
    console.warn("Invalid time value:", seconds);
    return "00:00:00";
  }
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
}

//---- å¼¹ UI ç›¸å…³å‡½æ•° ----

const keyNamePopup = "whisper--popup"; // å¼¹å‡ºçª—çš„ key åç§°

// æ›´æ–°æ¶ˆæ¯
// å½“å¼¹å‡ºçª—å£æ˜¾ç¤ºæ—¶ï¼Œå¦‚æœæƒ³ä¸­é€”æ›´æ”¹æ¶ˆæ¯ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å‡½æ•°
const updatePopupUI = (messageHTML: string) => {
  const messageEl = parent.document.getElementById("whisperSubtitles--message") as HTMLDivElement | null;
  if (messageEl) messageEl.innerHTML = messageHTML; // å¦‚æœå¼¹å‡ºçª—å£å·²æ˜¾æ›´æ–°æ¶ˆæ¯
  else popupUI(messageHTML); // å¦‚æœå¼¹å‡ºçª—å£æœªç¤ºï¼Œåˆ›å»ºå¸¦æœ‰æ¶ˆæ¯çš„å¼¹å‡ºçª—å£
};

// åˆ›å»ºå¼¹å‡ºçª—å£
const popupUI = (printMain: string, targetBlockUuid?: string) => {
  // dot select
  const dotSelect = targetBlockUuid ? `
  &#root>div {
    &.light-theme>main>div span#dot-${targetBlockUuid}{
        outline: 2px solid var(--ls-link-ref-text-color);
    }
    &.dark-theme>main>div span#dot-${targetBlockUuid}{
        outline: 2px solid aliceblue;
    }
  }
  ` : "";
  logseq.provideUI({
    attrs: {
      title: "Whisper subtitles plugin",
    },
    key: keyNamePopup,
    reset: true,
    style: {
      width: "330px", // width
      minHeight: "220px", // min-height
      maxHeight: "400px", // max-height
      overflowY: "auto",
      left: "unset",
      bottom: "unset",
      right: "1em",
      top: "4em",
      paddingLeft: "2em",
      paddingTop: "2em",
      backgroundColor: 'var(--ls-primary-background-color)',
      color: 'var(--ls-primary-text-color)',
      boxShadow: '1px 2px 5px var(--ls-secondary-background-color)',
    },
    template: `
        <div title="">
            <p>Whisper subtitles ${t("plugin")} <button class="button" id="whisperSubtitles--showSettingsUI" title="${t("plugin settings")}">âš™ï¸</button></p>
            <div id="whisperSubtitles--message">
            ${printMain}
            </div>
        </div>
        <style>
      body>div {
        ${dotSelect}
        &#${logseq.baseInfo.id}--${keyNamePopup} {
          & button.button#whisperSubtitles--showSettingsUI {
            display: unset;
          }
          & div#whisperSubtitles--message {
            &>div#whisper-subtitles-loader-container {
              display: flex;
              justify-content: center;
              align-items: center;
              flex-direction: column;
              width: 100px;
              height: 100px;
              &>div#whisper-subtitles-loader {
                border: 15px solid #39d4ff;
                border-radius: 50%;
                width: 60px;
                height: 60px;
                animation: spin 2s linear infinite;
              }
              &>p[data-message="processing"] {
                font-size: 1.2em;
                line-height: 1.5em;
              }
            }
          }
        }
      }
        @keyframes spin{
          0%{
            transform: rotate(0deg);
          }
          50%{
            transform: rotate(180deg);
            border-radius: 0%;
            width: 20px;
            height: 20px;
            border: 5px double #061fd5;
          }
          100%{
            transform: rotate(360deg);
          }
        }
        </style>
        `,
  });
  setTimeout(() => {
    //plugin settings button
    const showSettingsUI = parent.document.getElementById("whisperSubtitles--showSettingsUI") as HTMLButtonElement | null;
    if (showSettingsUI) showSettingsUI.addEventListener("click", () => logseq.showSettingsUI(), { once: true });
  }, 50);
};

// æ˜¾ç¤ºå¤„ç†çš„ UI
const showProcessingUI = (blockUuid: string) => {
  popupUI(`
    <div id="whisper-subtitles-loader-container">
      <div id="whisper-subtitles-loader"></div>
      <p data-message="processing">${t("Processing...")}</p>
    </div>
    <p>${t("It will take a few minutes.")}</p>
  `, blockUuid);
};

// éšè—å¤„ç†ä¸­çš„ UI
const hideProcessingUI = () => {
  removePopupUI();
};

// ä» DOM ä¸­é™¤å¼¹å‡ºçª—å£
const removePopupUI = () => parent.document.getElementById(logseq.baseInfo.id + "--" + keyNamePopup)?.remove();

//---- å¼¹å‡º UI ç›¸å…³å‡½æ•°ç»“æŸ ----

// æ·»åŠ ä¸€ä¸ªæ–°å‡½æ•°æ¥å¤„ç† OpenAI API è½®è¯¢çš„æç¤º
function handleOpenAIRotationNotification(message: string) {
  const regex = /Using OpenAI API setting (\d+)/;
  const match = message.match(regex);
  if (match) {
    const number = match[1];
    logseq.UI.showMsg(`Using OpenAI API setting ${number}`, 'info');
  }
}

console.log("Plugin script finished loading, calling logseq.ready");
logseq.ready(main).catch(console.error);

logseq.provideModel({
  getSetting(key: string) {
    return logseq.settings[key];
  },
  getAllSettings() {
    return logseq.settings;
  }
});

function showHotwordsInputDialog(): Promise<string> {
  return new Promise((resolve) => {
    let timeoutId: NodeJS.Timeout;

    logseq.provideUI({
      key: 'hotwords-input',
      reset: true,
      template: `
        <div class="hotwords-input-container">
          <h3>Enter Hotwords</h3>
          <p>Please enter hotwords separated by commas(auto close after 3 minutes):</p>
          <textarea id="hotwords-input" rows="4" cols="50"></textarea>
          <div class="button-container">
            <button id="submit-hotwords">Submit</button>
            <button id="cancel-hotwords">Cancel</button>
          </div>
        </div>
      `,
      style: {
        width: '400px',
        height: 'auto',
        backgroundColor: 'var(--ls-primary-background-color)',
        color: 'var(--ls-primary-text-color)',
        padding: '20px',
        borderRadius: '8px',
        boxShadow: '0 2px 10px rgba(0, 0, 0, 0.1)',
      },
    });

    const submitHotwords = () => {
      clearTimeout(timeoutId);
      const textarea = parent.document.getElementById('hotwords-input') as HTMLTextAreaElement;
      const hotwords = textarea.value.trim();
      logseq.provideUI({
        key: 'hotwords-input',
        reset: true,
      });
      resolve(hotwords);
    };

    setTimeout(() => {
      const submitButton = parent.document.getElementById('submit-hotwords');
      const cancelButton = parent.document.getElementById('cancel-hotwords');
      const textarea = parent.document.getElementById('hotwords-input') as HTMLTextAreaElement;

      if (submitButton && cancelButton && textarea) {
        submitButton.addEventListener('click', submitHotwords);

        cancelButton.addEventListener('click', () => {
          clearTimeout(timeoutId);
          logseq.provideUI({
            key: 'hotwords-input',
            reset: true,
          });
          resolve('');
        });

        // è®¾ç½®5åˆ†é’Ÿåè‡ªåŠ¨æäº¤
        timeoutId = setTimeout(submitHotwords, 3 * 60* 1000);
      }
    }, 100);
  });
}

async function transcribeFromSubtitleFile(b: IHookEvent, performSegmentation: boolean = false, outputFormat: 'youtube' | 'other' = 'other') {
  const currentBlock = await logseq.Editor.getBlock(b.uuid);
  if (currentBlock) {
    try {
      console.log("Block content:", currentBlock.content);
      
      showProcessingUI(currentBlock.uuid);

      const subtitleFilePath = extractSubtitleFilePath(currentBlock.content);
      console.log("Extracted subtitle file path:", subtitleFilePath);
      
      if (!subtitleFilePath) {
        throw new Error("No subtitle file path found in the block content");
      }

      const settings = getTranscriptionSettings();

      const transcription = await convertSubtitleToTranscription(subtitleFilePath);

      transcription.source = outputFormat;
      console.log("Transcription:", transcription);

      let result = transcription.segments;

      if (performSegmentation) {
        const baseEndpoint = logseq.settings?.whisperLocalEndpoint || "http://127.0.0.1:5014";
        const endpoint = `${baseEndpoint}/segment`;
        
        const formData = new FormData();
        formData.append('text', transcription.segments.map(seg => `{{timestamp ${seg.start}}} ${seg.text}`).join('\n'));
        appendCommonFormData(formData, settings);

        const response = await fetch(endpoint, {
          method: 'POST',
          body: formData,
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);
        }

        const segmentationResult = await response.json();
        result = segmentationResult.segments;
      }

      console.log("Final result:", result);

      await insertTranscription(currentBlock, { segments: result, source: outputFormat });

      logseq.UI.showMsg(t("Transcription from subtitle file completed and inserted"), "success");
    } catch (e: any) {
      console.error("Error in transcribeFromSubtitleFile:", e);
      logseq.UI.showMsg(t("Transcription from subtitle file failed: ") + e.message, "error");
    } finally {
      hideProcessingUI();
    }
  }
}

function extractSubtitleFilePath(content: string): string | null {
  // åŒ¹é… orgmode é“¾æ¥æ ¼å¼
  const logseqLinkMatch = content.match(/\[\[(.*?\.(?:srt|ass))\]\[.*?\]\]/);
  if (logseqLinkMatch) {
    return logseqLinkMatch[1];
  }

  // åŒ¹é…æ™®é€šçš„ Markdown é“¾æ¥æ ¼å¼ï¼ˆä¿ç•™åŸæœ‰çš„åŒ¹é…é€»è¾‘ï¼‰
  const markdownLinkMatch = content.match(/\[(.*?)\]\((.*?\.(?:srt|ass))\)/);
  if (markdownLinkMatch) {
    return markdownLinkMatch[2];
  }

  return null;
}

async function convertSubtitleToTranscription(filePath: string): Promise<TranscriptionResponse> {
  console.log("Converting subtitle file to transcription:", filePath);
  
  const baseEndpoint = logseq.settings?.whisperLocalEndpoint || "http://127.0.0.1:5014";
  const endpoint = `${baseEndpoint}/convert_subtitle`;
  
  const formData = new FormData();
  formData.append('subtitle_path', filePath);
  formData.append('graph_path', logseq.settings?.graphPath || '');

  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);
    }

    const result = await response.json();
    console.log("Conversion result:", result);

    return result;
  } catch (error) {
    console.error("Error converting subtitle file:", error);
    throw error;
  }
}

async function segmentTranscription(transcription: TranscriptionResponse, settings: TranscriptionOptions): Promise<TranscriptionResponse> {
  const fullText = transcription.segments.map(seg => `{{timestamp ${seg.start}}} ${seg.text}`).join('\n');
  const baseEndpoint = logseq.settings?.whisperLocalEndpoint || "http://127.0.0.1:5014";
  const endpoint = `${baseEndpoint}/segment`;
  
  const formData = new FormData();
  formData.append('text', fullText);
  appendCommonFormData(formData, settings);

  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      console.error("Server response:", response.status, response.statusText);
      const responseText = await response.text();
      console.error("Response body:", responseText);
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const result = await response.json();

    // å¤„ç† OpenAI API è½®è¯¢é€šçŸ¥
    if (result.openai_rotation_message) {
      handleOpenAIRotationNotification(result.openai_rotation_message);
    }

    console.log("Segmentation result:", result);

    // å°†åˆ†æ®µåçš„æ–‡æœ¬è½¬æ¢å› TranscriptionResponse æ ¼å¼
    const segmentedTranscription: TranscriptionResponse = {
      segments: result.segments.map((segment: string) => {
        const match = segment.match(/^\{\{timestamp (\d+)\}\} (.+)$/);
        if (match) {
          return {
            start: parseInt(match[1]),
            text: match[2]
          };
        }
        return null;
      }).filter((segment: any) => segment !== null)
    };

    return segmentedTranscription;
  } catch (error) {
    console.error("Segmentation error:", error);
    throw error;
  }
}

async function segment_text_with_openai(text: string, settings: TranscriptionOptions): Promise<string[]> {
  const baseEndpoint = logseq.settings?.whisperLocalEndpoint || "http://127.0.0.1:5014";
  const endpoint = `${baseEndpoint}/segment`;
  
  const formData = new FormData();
  formData.append('text', text);
  appendCommonFormData(formData, settings);

  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const result = await response.json();
    return result.segments;
  } catch (error) {
    console.error("Segmentation error:", error);
    throw error;
  }
}

async function summarizeSubtitle(e: IHookEvent) {
  console.log("Starting summarizeSubtitle function");
  console.log("Event:", e);

  try {
    const block = await logseq.Editor.getBlock(e.uuid);
    console.log("Retrieved block:", block);

    if (!block) {
      throw new Error("Failed to retrieve block");
    }

    const settings = getTranscriptionSettings();
    const summarizeBlockContent = settings.summarizeBlockContent || "#è§†é¢‘æ€»ç»“";
    const summarizeOpenAISettingPriority = settings.summarizeOpenAISettingPriority || "1,2,3,4,5";

    console.log("Settings:", {
      summarizeBlockContent,
      summarizeOpenAISettingPriority
    });

    // æ˜¾ç¤ºå¤„ç†ä¸­çš„ UI
    showProcessingUI(block.uuid);

    // è·å–å—å†…å®¹ï¼ˆåŒ…æ‹¬å­å—ï¼‰
    const content = await getBlockContentRecursively(block);
    console.log("Retrieved content:", content);

    // è°ƒç”¨åç«¯ API è¿›è¡Œæ€»ç»“
    const summary = await summarizeContent(content, summarizeOpenAISettingPriority);
    console.log("Generated summary:", summary);

    // åˆ›å»ºæ€»ç»“å—
    const summaryBlock = await logseq.Editor.insertBlock(block.uuid, summarizeBlockContent, { sibling: true });
    console.log("Created summary block:", summaryBlock);

    if (summaryBlock) {
      await logseq.Editor.insertBlock(summaryBlock.uuid, summary);
      console.log("Inserted summary content");
    } else {
      throw new Error("Failed to create summary block");
    }

    logseq.UI.showMsg(t("Summary created successfully"), "success");
  } catch (error) {
    console.error("Error in summarizeSubtitle:", error);
    logseq.UI.showMsg(t("Failed to create summary: ") + (error instanceof Error ? error.message : String(error)), "error");
  } finally {
    // éšè—å¤„ç†ä¸­çš„ UI
    hideProcessingUI();
  }
}

async function getBlockContentRecursively(block: any): Promise<string> {
  console.log("Starting getBlockContentRecursively for block:", block);
  let content = block.content.replace(/\{\{.*?\}\}/g, ''); // ç§»é™¤æ—¶é—´æˆ³
  if (block.children) {
    for (const childId of block.children) {
      try {
        console.log("Attempting to retrieve child block with ID:", childId);
        // æ£€æŸ¥ childId æ˜¯å¦æ˜¯æ•°ç»„ï¼Œå¦‚æœæ˜¯ï¼Œå–ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆå®é™…çš„ UUIDï¼‰
        const actualChildId = Array.isArray(childId) ? childId[1] : childId;
        
        // æ£€æŸ¥ actualChildId æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ UUID
        if (typeof actualChildId === 'string' && /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i.test(actualChildId)) {
          const childBlock = await logseq.Editor.getBlock(actualChildId);
          if (childBlock) {
            console.log("Successfully retrieved child block:", childBlock);
            content += '\n' + await getBlockContentRecursively(childBlock);
          } else {
            console.warn("Child block not found for ID:", actualChildId);
          }
        } else {
          console.warn("Invalid child block ID:", actualChildId);
        }
      } catch (error) {
        console.error("Error retrieving child block:", error);
        if (error instanceof Error) {
          console.error("Error message:", error.message);
          console.error("Error stack:", error.stack);
        }
        // ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå­å—ï¼Œè€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
      }
    }
  }
  console.log("Finished getBlockContentRecursively, content length:", content.length);
  return content;
}

async function summarizeContent(content: string, apiSettingPriority: string): Promise<string> {
  console.log("Starting summarizeContent, content length:", content.length, "apiSettingPriority:", apiSettingPriority);
  const baseEndpoint = logseq.settings?.whisperLocalEndpoint || "http://127.0.0.1:5014";
  const endpoint = `${baseEndpoint}/summarize`;
  
  const settings = getTranscriptionSettings();
  const formData = new FormData();
  formData.append('text', content);
  formData.append('api_setting_priority', apiSettingPriority);
  appendCommonFormData(formData, settings);

  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const result = await response.json();
    console.log("Summarization result:", result);
    return result.summary;
  } catch (error) {
    console.error("Error in summarizeContent:", error);
    throw error;
  }
}